{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cluster.dev - Kubernetes infrastructures in minutes! \u00b6 What is it? \u00b6 Cluster.dev is the cloud-native infrastructure orchestration framework. It is an open-source system delivered as a runtime inside Docker container. It is used for creating and managing Kubernetes clusters along with cloud resources like networks, domains and users with pre-defined Terraform modules. The orchestration is performed with simple manifests by GitOps approach and is designed to run inside the GitHub/GitLab/BitBucket pipelines. Resulting infrastructures have a \"ready to use\" Continuous Deployment systems that could deploy manifests, Helm charts and Kustomize using ArgoCD. Best-in-class automation and proven practices guarantee availability, scalability, and compliance with the most demanding data security and privacy standards. Designed for developers who are bored to configure cloud-native stack and just need infrastructure in code, kubeconfig, CD, dashboard, logging and monitoring out-of-the-box. Principle diagram \u00b6 How it works \u00b6 In the background: Infrastructures are described as simple infrastructure manifests and are stored in a Git repository. Infrastructure changes are watched by GitHub/GitLab/Bitbucket pipeline and trigger the launch of the reconciler tool. Reconciler tool generates Terraform variables files and performs ordered invoking for the modules. Terraform creates a \"state bucket\" in your Cloud Provider account where all infrastructure objects and configs are stored. Typically it is defined on Cloud Object Storage like AWS S3. Terraform modules create Minikube/EKS/GKE/etc.. cluster, VPC and DNS zone within your Cloud Provider. Kubernetes addons module deploys: Ingress controller, Cert-Manager, External DNS, ArgoCD, Keycloak, etc.. ArgoCD continuous deployment system watches remote Git repositories and deploys your applications from raw manifests, Helm charts or Kustomize yamls. You receive: Automatically generated kubeconfig, ArgoCD UI URL's. Pre-configured: VPC, Networks, Domains, Security groups, Users, etc.. Deployed inside Kubernetes: Ingress Load Balancers, Kubernetes Dashboard, Logging (ELK), Monitoring (Prometheus/Grafana). Features \u00b6 Based on DevOps and SRE best-practices. Simple CI/CD integration. GitOps cluster management and application delivery. Automated provisioning of Kubernetes clusters in AWS, DO and GCE(in progress). Roadmap \u00b6 The cluster.dev project is in Alpha Stage. You can check its progress and upcoming features on the roadmap page .","title":"Welcome"},{"location":"#clusterdev-kubernetes-infrastructures-in-minutes","text":"","title":"Cluster.dev - Kubernetes infrastructures in minutes!"},{"location":"#what-is-it","text":"Cluster.dev is the cloud-native infrastructure orchestration framework. It is an open-source system delivered as a runtime inside Docker container. It is used for creating and managing Kubernetes clusters along with cloud resources like networks, domains and users with pre-defined Terraform modules. The orchestration is performed with simple manifests by GitOps approach and is designed to run inside the GitHub/GitLab/BitBucket pipelines. Resulting infrastructures have a \"ready to use\" Continuous Deployment systems that could deploy manifests, Helm charts and Kustomize using ArgoCD. Best-in-class automation and proven practices guarantee availability, scalability, and compliance with the most demanding data security and privacy standards. Designed for developers who are bored to configure cloud-native stack and just need infrastructure in code, kubeconfig, CD, dashboard, logging and monitoring out-of-the-box.","title":"What is it?"},{"location":"#principle-diagram","text":"","title":"Principle diagram"},{"location":"#how-it-works","text":"In the background: Infrastructures are described as simple infrastructure manifests and are stored in a Git repository. Infrastructure changes are watched by GitHub/GitLab/Bitbucket pipeline and trigger the launch of the reconciler tool. Reconciler tool generates Terraform variables files and performs ordered invoking for the modules. Terraform creates a \"state bucket\" in your Cloud Provider account where all infrastructure objects and configs are stored. Typically it is defined on Cloud Object Storage like AWS S3. Terraform modules create Minikube/EKS/GKE/etc.. cluster, VPC and DNS zone within your Cloud Provider. Kubernetes addons module deploys: Ingress controller, Cert-Manager, External DNS, ArgoCD, Keycloak, etc.. ArgoCD continuous deployment system watches remote Git repositories and deploys your applications from raw manifests, Helm charts or Kustomize yamls. You receive: Automatically generated kubeconfig, ArgoCD UI URL's. Pre-configured: VPC, Networks, Domains, Security groups, Users, etc.. Deployed inside Kubernetes: Ingress Load Balancers, Kubernetes Dashboard, Logging (ELK), Monitoring (Prometheus/Grafana).","title":"How it works"},{"location":"#features","text":"Based on DevOps and SRE best-practices. Simple CI/CD integration. GitOps cluster management and application delivery. Automated provisioning of Kubernetes clusters in AWS, DO and GCE(in progress).","title":"Features"},{"location":"#roadmap","text":"The cluster.dev project is in Alpha Stage. You can check its progress and upcoming features on the roadmap page .","title":"Roadmap"},{"location":"CONTRIBUTING/","text":"Contributing \u00b6 If you plan to make some changes, please have a look at our style guide requirements first. If you want to add another Cloud Provider or Kubernetes Provisioner, please see add-provider-provisioner.md . How to contribute \u00b6 1) Create an issue that you are going to address in GH Issues , for example issue #3 . 2) Spawn new branch from master named with the GH Issue you are going to address: feature/GH-3 . For preferable branch naming format see here . 3) To start a new cluster corresponding to your issue, create a manifest file in .cluster.dev/gh-3.yaml , setting the name with the target issue: installed : true name : gh-3 #CHANGE ME provider : type : aws region : eu-central-1 availability_zones : - eu-central-1b - eu-central-1c vpc : default domain : cluster.dev provisioner : type : minikube instanceType : m5.large addons : nginx-ingress : true cert-manager : true apps : - /kubernetes/apps/samples 4) Create a new workflow in .github/workflows and name it corresponding to your issue: gh-3.yaml . Set the required branch and file name in placeholders (marked with #CHANGE ME ), for example: on : push : branches : - feature/GH-3 #CHANGE ME jobs : deploy_cluster_job : runs-on : ubuntu-latest name : Deploy and Update K8s Clusters steps : - name : Checkout Repo uses : actions/checkout@v2 with : ref : 'feature/GH-3' #CHANGE ME - name : Reconcile Clusters id : reconcile uses : shalb/cluster.dev@v0.3.3 #CHANGE ME env : AWS_ACCESS_KEY_ID : \"${{ secrets.AWS_ACCESS_KEY_ID }}\" AWS_SECRET_ACCESS_KEY : \"${{ secrets.AWS_SECRET_ACCESS_KEY }}\" CLUSTER_CONFIG_PATH : \"./.cluster.dev/\" VERBOSE_LVL : DEBUG 5) Commit and push both files with the comment, for example: GH-3 Initial Commit . GitHub automatically creates reference to the related issue to let other contributors know that related work has been addressed somewhere else. 6) Check the logs in GH Actions to track the environment building process. To do this, choose your branch in the workflows section and choose your last build: 7) Check the cluster status with your target cloud provider. 8) After you have made all the necessary changes, open a Pull Request and assign it to @voatsap or @MaxymVlasov for the review. 9) After successful review, squash and merge your PR to master with the included comment Resolve GH-3 . 10) After merging be sure to delete all the resources associated with the issue (EC2 instances, Elastic IP's etc.) that have been used for testing.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"If you plan to make some changes, please have a look at our style guide requirements first. If you want to add another Cloud Provider or Kubernetes Provisioner, please see add-provider-provisioner.md .","title":"Contributing"},{"location":"CONTRIBUTING/#how-to-contribute","text":"1) Create an issue that you are going to address in GH Issues , for example issue #3 . 2) Spawn new branch from master named with the GH Issue you are going to address: feature/GH-3 . For preferable branch naming format see here . 3) To start a new cluster corresponding to your issue, create a manifest file in .cluster.dev/gh-3.yaml , setting the name with the target issue: installed : true name : gh-3 #CHANGE ME provider : type : aws region : eu-central-1 availability_zones : - eu-central-1b - eu-central-1c vpc : default domain : cluster.dev provisioner : type : minikube instanceType : m5.large addons : nginx-ingress : true cert-manager : true apps : - /kubernetes/apps/samples 4) Create a new workflow in .github/workflows and name it corresponding to your issue: gh-3.yaml . Set the required branch and file name in placeholders (marked with #CHANGE ME ), for example: on : push : branches : - feature/GH-3 #CHANGE ME jobs : deploy_cluster_job : runs-on : ubuntu-latest name : Deploy and Update K8s Clusters steps : - name : Checkout Repo uses : actions/checkout@v2 with : ref : 'feature/GH-3' #CHANGE ME - name : Reconcile Clusters id : reconcile uses : shalb/cluster.dev@v0.3.3 #CHANGE ME env : AWS_ACCESS_KEY_ID : \"${{ secrets.AWS_ACCESS_KEY_ID }}\" AWS_SECRET_ACCESS_KEY : \"${{ secrets.AWS_SECRET_ACCESS_KEY }}\" CLUSTER_CONFIG_PATH : \"./.cluster.dev/\" VERBOSE_LVL : DEBUG 5) Commit and push both files with the comment, for example: GH-3 Initial Commit . GitHub automatically creates reference to the related issue to let other contributors know that related work has been addressed somewhere else. 6) Check the logs in GH Actions to track the environment building process. To do this, choose your branch in the workflows section and choose your last build: 7) Check the cluster status with your target cloud provider. 8) After you have made all the necessary changes, open a Pull Request and assign it to @voatsap or @MaxymVlasov for the review. 9) After successful review, squash and merge your PR to master with the included comment Resolve GH-3 . 10) After merging be sure to delete all the resources associated with the issue (EC2 instances, Elastic IP's etc.) that have been used for testing.","title":"How to contribute"},{"location":"DESIGN/","text":"Product Design \u00b6 Infrastructure Concept \u00b6 Single infrastructure should be described as one yaml manifest. Each infrastructure should contain: Infrastructure state, configs and secrets storage. Private network definition. DNS zone pointed to cluster resources. One Kubernetes cluster. Continuous Deployment tool for Kubernetes applications. User Management system. Infrastructures are deployed and reconciled with application delivered as Docker container. Reconciliation should follow GitOps approach - follow updates on target git repo. Single infrastructure repo could contain: Multiple infrastructure declarations. Common and infrastructure-dependent customer defined Terraform modules. Modules could be sources from external repos. Module definitions could be templated with values from yaml manifest Common and infrastructure dependent Kubernetes applications. Applications represented as a ArgoCD applications. Application definitions could be templated. Using ArgoCD application should be possible deploy any helm/kustomize/raw-manifest from external repos. Each infrastructure should have single admin user with full privileges created. Keycloak is used for user/group management and adding external providers and SSO. Project structure \u00b6 Terraform modules : For each cloud provider we create own set of infrastructure modules: - backend Storage for Terraform state files, kubernetes configs and secrets. - vpc Module used for creating or re-using virtual private network. - domain Module used for creating or re-using dns zone for infrastructure. - kubernetes Module for deploying Kubernetes cluster. - addons Module for deploying additional applications inside Kubernetes cluster. Go-based reconciler - that generates variables and performs ordered module invocation. Bash-based reconciler (would be deprecated) Kubernetes Addons (ingress, cert-manager, external-dns, ArgoCD, Keycloak, etc..) Domain Service for creating custom Domains SaaS for managing infrastructure using Web UI.","title":"Design"},{"location":"DESIGN/#product-design","text":"","title":"Product Design"},{"location":"DESIGN/#infrastructure-concept","text":"Single infrastructure should be described as one yaml manifest. Each infrastructure should contain: Infrastructure state, configs and secrets storage. Private network definition. DNS zone pointed to cluster resources. One Kubernetes cluster. Continuous Deployment tool for Kubernetes applications. User Management system. Infrastructures are deployed and reconciled with application delivered as Docker container. Reconciliation should follow GitOps approach - follow updates on target git repo. Single infrastructure repo could contain: Multiple infrastructure declarations. Common and infrastructure-dependent customer defined Terraform modules. Modules could be sources from external repos. Module definitions could be templated with values from yaml manifest Common and infrastructure dependent Kubernetes applications. Applications represented as a ArgoCD applications. Application definitions could be templated. Using ArgoCD application should be possible deploy any helm/kustomize/raw-manifest from external repos. Each infrastructure should have single admin user with full privileges created. Keycloak is used for user/group management and adding external providers and SSO.","title":"Infrastructure Concept"},{"location":"DESIGN/#project-structure","text":"Terraform modules : For each cloud provider we create own set of infrastructure modules: - backend Storage for Terraform state files, kubernetes configs and secrets. - vpc Module used for creating or re-using virtual private network. - domain Module used for creating or re-using dns zone for infrastructure. - kubernetes Module for deploying Kubernetes cluster. - addons Module for deploying additional applications inside Kubernetes cluster. Go-based reconciler - that generates variables and performs ordered module invocation. Bash-based reconciler (would be deprecated) Kubernetes Addons (ingress, cert-manager, external-dns, ArgoCD, Keycloak, etc..) Domain Service for creating custom Domains SaaS for managing infrastructure using Web UI.","title":"Project structure"},{"location":"OPTIONS/","text":"Options List for Cluster Manifests \u00b6 The section contains a list of options that are set in cluster manifests (yaml files) to configure the clusters. Manifest Example \u00b6 For more examples please see the /.cluster.dev directory. # .cluster.dev/staging.yaml cluster : installed : true name : staging cloud : provider : aws region : eu-central-1 vpc : default domain : cluster.dev provisioner : type : minikube instanceType : m5.large addons : nginx-ingress : true cert-manager : true apps : - /kubernetes/apps/samples Global Options \u00b6 Key Required Type Values Default Description installed - bool false , true true Defines if the cluster should be deployed or deleted, false would delete the existing cluster. name + string any Cluster name to be used across all resources. provider.type + string aws , digitalocean Defines a cloud provider. provider.region + string ex: us-east-1 , ams3 Defines a cloud provider region, in which to create the cluster. provider. availability_zone - string ex: us-east-1b , us-east-1c cloud.region'a' , cloud.region'b' Define networks and nodes location inside a single region. Minimum two zones should be defined. Cluster nodes could be spread across different datacenters. Multiple zones provide high availability, however, can lead to cost increase. provider.domain - string FQDN ex: cluster.dev , example.org cluster.dev To expose cluster resources, the DNS zone is required. If set to cluster.dev , the installer would create a zone cluster-name-organization.cluster.dev and point it to your cloud service NS'es. Alternate, you can set your own zone, which already exists in the target cloud. provider.vpc - string default , create , vpc_id default Virtual Private Cloud. - default - use default one - create - installer creates a new VPC - vpc_id - define an already existent (in AWS tag the networks manually with the \"cluster.dev/subnet_type\" = \"private/public\" tags to make them usable). provider.vpc_cidr - string ex: 10.2.0.0/16 , 192.168.0.0/20 10.8.0.0/18 The CIDR block for the VPC. Cluster pods will use IPs from that pool. If you need peering between VPCs, their CIDRs should be unique. Cluster Provisioners \u00b6 Amazon AWS Provisioners \u00b6 Required environment variables to be passed to the container: AWS_ACCESS_KEY_ID - the access key ID required for user programmatic access, see the AWS documentation . AWS_SECRET_ACCESS_KEY - the secret access key required for user programmatic access. Key Required Type Values Default Description provisioner.type + string minikube , eks Provisioner to deploy the cluster with. AWS Minikube \u00b6 example yaml file: .cluster.dev/aws-minikube.yaml Key Required Type Values Default Description provisioner. instanceType + string ex: m5.xlarge m5.large Single node Kubernetes cluster AWS EC2 instance type. AWS EKS \u00b6 example yaml file: .cluster.dev/aws-eks.yaml Key Required Type Values Default Description provisioner.version + string ex: 1.15 , 1.16 1.16 Kubernetes version. provisioner. additional_security_group_ids - list ex: sg-233ba1, sg-2221bb A list of additional security group IDs to include in worker launch config. provisioner.node_group. name - string ex: spot-group , on-demand-group node-group Name for Kubernetes group of worker nodes. provisioner.node_group. type - string ex: spot , on-demand on-demand Type for Kubernetes group of worker nodes. provisioner.node_group. instance_type - string ex: t3.medium , c5.xlarge m5.large Size of a Kubernetes worker node. provisioner.node_group. asg_desired_capacity - integer 1 ..<= asg_max_size 1 Desired worker capacity in the autoscaling group. provisioner.node_group. asg_max_size - integer 1 .. cloud limit 3 Maximum worker capacity in the autoscaling group. provisioner.node_group. asg_min_size - integer 1 ..<= asg_max_size 1 Minimum worker capacity in the autoscaling group. provisioner.node_group. root_volume_size - integer 20 .. cloud limit 40 Root volume size in GB in worker instances. provisioner.node_group. kubelet_extra_args - string --node-labels= kubernetes.io/ lifecycle=spot This string is passed directly to kubelet, if set. Useful for adding labels or taints. provisioner.node_group. override_instance_types - list ex: m5.large, m5a.large, c5.xlarge A list of override instance types for mixed instances policy. provisioner.node_group. spot_allocation_strategy - string lowest-price , capacity-optimized lowest-price - lowest-price - the Auto Scaling group launches instances using the Spot pools with the lowest price, and evenly allocates your instances across the number of Spot pools. - capacity-optimized - the Auto Scaling group launches instances using Spot pools that are optimally chosen based on the available Spot capacity. provisioner.node_group. spot_instance_pools - integer 1 .. cloud limit 10 Number of Spot pools per availability zone to allocate capacity. EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify. provisioner.node_group. spot_max_price - float \"\" \"\" Maximum price per unit hour that the user is willing to pay for the Spot instances. Default is the on-demand price. provisioner.node_group. on_demand_base_capacity - integer 0 .. 100 0 Absolute minimum amount of desired capacity that must be fulfilled by on-demand instances. provisioner.node_group. on_demand_percentage _above_base_capacity - integer 0 .. 100 0 Percentage split between on-demand and Spot instances above the base on-demand capacity. Advanced parameters can be found here . DigitalOcean Provisioners \u00b6 The DigitalOcean (DO) provider is used to interact with the resources supported by DigitalOcean. The following environment variables should be set: DIGITALOCEAN_TOKEN - the DO API token, see the DO documentation . SPACES_ACCESS_KEY_ID - the access key ID used for Spaces API operations, see the DO documentation . SPACES_SECRET_ACCESS_KEY - the secret access key used for Spaces API operations. Key Required Type Values Default Description provisioner.type + string managed-kubernetes Provisioner to deploy cluster with. provider.project - string ex: staging default DigitalOcean Project name. DigitalOcean Managed Kubernetes \u00b6 example yaml file: .cluster.dev/digitalocean-k8s.yaml Key Required Type Values Default Description version - string ex: 1.16 DigitalOcean managed Kubernetes version . nodeCount + integer 1-512 1 Number of Droplet instances in the cluster. nodeSize + string ex: s-4vcpu-8gb s-1vcpu-2gb The slug identifier for the type of Droplets used as workers in the node pool. autoScale - boolean true , false false A boolean indicating whether auto-scaling is enabled. minNodes - boolean 1-512 1 If autoScale is enabled, defines a minimum number of Droplet instances in the cluster. maxNodes - boolean 1-512 1 If autoScale is enabled, defines a maximum number of Droplet instances in the cluster. Cluster Addons \u00b6 Key Required Type Values Default Description nginx-ingress - boolean true , false true Deploy nginx-ingress . cert-manager - boolean true , false true Deploy cert-manager . external-dns - boolean true , false true Deploy external-dns . argo-cd - boolean true , false true Deploy argo-cd . olm - boolean true , false true Deploy Operator Lifecycle Manager . keycloak - boolean true , false true Deploy Keycloak Operator .","title":"Options List for Cluster Manifests"},{"location":"OPTIONS/#options-list-for-cluster-manifests","text":"The section contains a list of options that are set in cluster manifests (yaml files) to configure the clusters.","title":"Options List for Cluster Manifests"},{"location":"OPTIONS/#manifest-example","text":"For more examples please see the /.cluster.dev directory. # .cluster.dev/staging.yaml cluster : installed : true name : staging cloud : provider : aws region : eu-central-1 vpc : default domain : cluster.dev provisioner : type : minikube instanceType : m5.large addons : nginx-ingress : true cert-manager : true apps : - /kubernetes/apps/samples","title":"Manifest Example"},{"location":"OPTIONS/#global-options","text":"Key Required Type Values Default Description installed - bool false , true true Defines if the cluster should be deployed or deleted, false would delete the existing cluster. name + string any Cluster name to be used across all resources. provider.type + string aws , digitalocean Defines a cloud provider. provider.region + string ex: us-east-1 , ams3 Defines a cloud provider region, in which to create the cluster. provider. availability_zone - string ex: us-east-1b , us-east-1c cloud.region'a' , cloud.region'b' Define networks and nodes location inside a single region. Minimum two zones should be defined. Cluster nodes could be spread across different datacenters. Multiple zones provide high availability, however, can lead to cost increase. provider.domain - string FQDN ex: cluster.dev , example.org cluster.dev To expose cluster resources, the DNS zone is required. If set to cluster.dev , the installer would create a zone cluster-name-organization.cluster.dev and point it to your cloud service NS'es. Alternate, you can set your own zone, which already exists in the target cloud. provider.vpc - string default , create , vpc_id default Virtual Private Cloud. - default - use default one - create - installer creates a new VPC - vpc_id - define an already existent (in AWS tag the networks manually with the \"cluster.dev/subnet_type\" = \"private/public\" tags to make them usable). provider.vpc_cidr - string ex: 10.2.0.0/16 , 192.168.0.0/20 10.8.0.0/18 The CIDR block for the VPC. Cluster pods will use IPs from that pool. If you need peering between VPCs, their CIDRs should be unique.","title":"Global Options"},{"location":"OPTIONS/#cluster-provisioners","text":"","title":"Cluster Provisioners"},{"location":"OPTIONS/#amazon-aws-provisioners","text":"Required environment variables to be passed to the container: AWS_ACCESS_KEY_ID - the access key ID required for user programmatic access, see the AWS documentation . AWS_SECRET_ACCESS_KEY - the secret access key required for user programmatic access. Key Required Type Values Default Description provisioner.type + string minikube , eks Provisioner to deploy the cluster with.","title":"Amazon AWS Provisioners"},{"location":"OPTIONS/#aws-minikube","text":"example yaml file: .cluster.dev/aws-minikube.yaml Key Required Type Values Default Description provisioner. instanceType + string ex: m5.xlarge m5.large Single node Kubernetes cluster AWS EC2 instance type.","title":"AWS Minikube"},{"location":"OPTIONS/#aws-eks","text":"example yaml file: .cluster.dev/aws-eks.yaml Key Required Type Values Default Description provisioner.version + string ex: 1.15 , 1.16 1.16 Kubernetes version. provisioner. additional_security_group_ids - list ex: sg-233ba1, sg-2221bb A list of additional security group IDs to include in worker launch config. provisioner.node_group. name - string ex: spot-group , on-demand-group node-group Name for Kubernetes group of worker nodes. provisioner.node_group. type - string ex: spot , on-demand on-demand Type for Kubernetes group of worker nodes. provisioner.node_group. instance_type - string ex: t3.medium , c5.xlarge m5.large Size of a Kubernetes worker node. provisioner.node_group. asg_desired_capacity - integer 1 ..<= asg_max_size 1 Desired worker capacity in the autoscaling group. provisioner.node_group. asg_max_size - integer 1 .. cloud limit 3 Maximum worker capacity in the autoscaling group. provisioner.node_group. asg_min_size - integer 1 ..<= asg_max_size 1 Minimum worker capacity in the autoscaling group. provisioner.node_group. root_volume_size - integer 20 .. cloud limit 40 Root volume size in GB in worker instances. provisioner.node_group. kubelet_extra_args - string --node-labels= kubernetes.io/ lifecycle=spot This string is passed directly to kubelet, if set. Useful for adding labels or taints. provisioner.node_group. override_instance_types - list ex: m5.large, m5a.large, c5.xlarge A list of override instance types for mixed instances policy. provisioner.node_group. spot_allocation_strategy - string lowest-price , capacity-optimized lowest-price - lowest-price - the Auto Scaling group launches instances using the Spot pools with the lowest price, and evenly allocates your instances across the number of Spot pools. - capacity-optimized - the Auto Scaling group launches instances using Spot pools that are optimally chosen based on the available Spot capacity. provisioner.node_group. spot_instance_pools - integer 1 .. cloud limit 10 Number of Spot pools per availability zone to allocate capacity. EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify. provisioner.node_group. spot_max_price - float \"\" \"\" Maximum price per unit hour that the user is willing to pay for the Spot instances. Default is the on-demand price. provisioner.node_group. on_demand_base_capacity - integer 0 .. 100 0 Absolute minimum amount of desired capacity that must be fulfilled by on-demand instances. provisioner.node_group. on_demand_percentage _above_base_capacity - integer 0 .. 100 0 Percentage split between on-demand and Spot instances above the base on-demand capacity. Advanced parameters can be found here .","title":"AWS EKS"},{"location":"OPTIONS/#digitalocean-provisioners","text":"The DigitalOcean (DO) provider is used to interact with the resources supported by DigitalOcean. The following environment variables should be set: DIGITALOCEAN_TOKEN - the DO API token, see the DO documentation . SPACES_ACCESS_KEY_ID - the access key ID used for Spaces API operations, see the DO documentation . SPACES_SECRET_ACCESS_KEY - the secret access key used for Spaces API operations. Key Required Type Values Default Description provisioner.type + string managed-kubernetes Provisioner to deploy cluster with. provider.project - string ex: staging default DigitalOcean Project name.","title":"DigitalOcean Provisioners"},{"location":"OPTIONS/#digitalocean-managed-kubernetes","text":"example yaml file: .cluster.dev/digitalocean-k8s.yaml Key Required Type Values Default Description version - string ex: 1.16 DigitalOcean managed Kubernetes version . nodeCount + integer 1-512 1 Number of Droplet instances in the cluster. nodeSize + string ex: s-4vcpu-8gb s-1vcpu-2gb The slug identifier for the type of Droplets used as workers in the node pool. autoScale - boolean true , false false A boolean indicating whether auto-scaling is enabled. minNodes - boolean 1-512 1 If autoScale is enabled, defines a minimum number of Droplet instances in the cluster. maxNodes - boolean 1-512 1 If autoScale is enabled, defines a maximum number of Droplet instances in the cluster.","title":"DigitalOcean Managed Kubernetes"},{"location":"OPTIONS/#cluster-addons","text":"Key Required Type Values Default Description nginx-ingress - boolean true , false true Deploy nginx-ingress . cert-manager - boolean true , false true Deploy cert-manager . external-dns - boolean true , false true Deploy external-dns . argo-cd - boolean true , false true Deploy argo-cd . olm - boolean true , false true Deploy Operator Lifecycle Manager . keycloak - boolean true , false true Deploy Keycloak Operator .","title":"Cluster Addons"},{"location":"ROADMAP/","text":"Project Roadmap \u00b6 v.0.1.x - Basic Scenario \u00b6 Create a state storage (AWS S3+Dynamo) for infrastructure resources Deploy a Kubernetes(Minikube) in AWS using default VPC Provision Kubernetes with addons: Ingress-Nginx, Load Balancer, Cert-Manager, ExtDNS, ArgoCD Deploy a sample \"WordPress\" application to Kubernetes cluster using ArgoCD Delivered as GitHub Actions and Docker Image v0.2.x - Bash-based PoC \u00b6 Deliver with cluster creation a default DNS sub-zone: *.username-clustername.cluster.dev Create a cluster.dev backend to register newly created clusters Support for GitLab CI Pipelines ArgoCD sample applications (raw manifests, local helm chart, public helm chart) Support for DigitalOcean Kubernetes cluster 59 DigitalOcean Domains sub-zones 65 AWS EKS provisioning. Spot and Mixed ASG support. Support for Operator Lifecycle Manager v0.3.x - Go-based Beta \u00b6 Go-based reconciler External secrets management with Sops and godaddy/kubernetes-external-secrets Team and user management with Keycloak Apps deployment: Kubernetes Dashboard, Grafana and Kibana. OIDC access to kubeconfig with Keycloak and jetstack/kube-oidc-proxy/ 53 SSO access to ArgoCD and base applications: Kubernetes Dashboard, Grafana, Kibana OIDC integration with GitHub, GitLab, Google Auth, Okta v0.4.x \u00b6 CLI Installer 54 Add GitHub runner and test GitHub Action Continuous Integration workflow Argo Workflows for DAG and CI tasks inside Kubernetes cluster Google Cloud Platform Kubernetes (GKE) support Custom Terraform modules and reconcilation Kind provisioner v0.5.x \u00b6 kops provisioner support k3s provisioner Cost $$$ estimation during installation Web user interface design v0.6.x \u00b6 Rancher RKE provisioner support Multi-cluster support for user management and SSO Multi-cluster support for ArgoCD Crossplane integration","title":"Roadmap"},{"location":"ROADMAP/#project-roadmap","text":"","title":"Project Roadmap"},{"location":"ROADMAP/#v01x-basic-scenario","text":"Create a state storage (AWS S3+Dynamo) for infrastructure resources Deploy a Kubernetes(Minikube) in AWS using default VPC Provision Kubernetes with addons: Ingress-Nginx, Load Balancer, Cert-Manager, ExtDNS, ArgoCD Deploy a sample \"WordPress\" application to Kubernetes cluster using ArgoCD Delivered as GitHub Actions and Docker Image","title":"v.0.1.x - Basic Scenario"},{"location":"ROADMAP/#v02x-bash-based-poc","text":"Deliver with cluster creation a default DNS sub-zone: *.username-clustername.cluster.dev Create a cluster.dev backend to register newly created clusters Support for GitLab CI Pipelines ArgoCD sample applications (raw manifests, local helm chart, public helm chart) Support for DigitalOcean Kubernetes cluster 59 DigitalOcean Domains sub-zones 65 AWS EKS provisioning. Spot and Mixed ASG support. Support for Operator Lifecycle Manager","title":"v0.2.x - Bash-based PoC"},{"location":"ROADMAP/#v03x-go-based-beta","text":"Go-based reconciler External secrets management with Sops and godaddy/kubernetes-external-secrets Team and user management with Keycloak Apps deployment: Kubernetes Dashboard, Grafana and Kibana. OIDC access to kubeconfig with Keycloak and jetstack/kube-oidc-proxy/ 53 SSO access to ArgoCD and base applications: Kubernetes Dashboard, Grafana, Kibana OIDC integration with GitHub, GitLab, Google Auth, Okta","title":"v0.3.x - Go-based Beta"},{"location":"ROADMAP/#v04x","text":"CLI Installer 54 Add GitHub runner and test GitHub Action Continuous Integration workflow Argo Workflows for DAG and CI tasks inside Kubernetes cluster Google Cloud Platform Kubernetes (GKE) support Custom Terraform modules and reconcilation Kind provisioner","title":"v0.4.x"},{"location":"ROADMAP/#v05x","text":"kops provisioner support k3s provisioner Cost $$$ estimation during installation Web user interface design","title":"v0.5.x"},{"location":"ROADMAP/#v06x","text":"Rancher RKE provisioner support Multi-cluster support for user management and SSO Multi-cluster support for ArgoCD Crossplane integration","title":"v0.6.x"},{"location":"add-new-docs/","text":"Add new documentation \u00b6 For add new documentation: Create .md file in /docs/ folder in the cluster.dev repo . Write documentation. Add new doc to mkdocs.yml where you thin it should be located. Create PR with kind/documentation label and assign reviewers from [CONTRIBUTING.md] + anastyakulyk .","title":"How to add a new docs"},{"location":"add-new-docs/#add-new-documentation","text":"For add new documentation: Create .md file in /docs/ folder in the cluster.dev repo . Write documentation. Add new doc to mkdocs.yml where you thin it should be located. Create PR with kind/documentation label and assign reviewers from [CONTRIBUTING.md] + anastyakulyk .","title":"Add new documentation"},{"location":"add-provider-provisioner/","text":"Adding a New Provider or Provisioner \u00b6 1. Create cluster manifest 2. Create Github Action and Gitlab pipeline 3. Define required treatment in main function 4. Define cloud related functions 5. Create cleanup function 6. Set cloud provisioner type 7. Deploy mandatory addons 8. Test build 1. Create cluster manifest \u00b6 Create a sample yaml cluster manifest with declaration for possible options in yaml and its naming. Naming for the options should be aligned with correspondent names in terraform provider or module. Set the list of options in OPTIONS.md . 2. Create Github Action and Gitlab pipeline \u00b6 Defining required cloud authentication credentials, like Username/Password, Personal Access Tokens, or Access files. 3. Define required treatment in main function \u00b6 Example: # entrypoint.sh digitalocean ) DEBUG \"Cloud Provider: DigitalOcean\" ;; 4. Define cloud related functions \u00b6 Define cloud related functions in a dedicated file, for ex: /bin/digitalocean_common.sh . Required functions: Function name What the function should do init_state_bucket check if the bucket exists and create a storage for terraform state init_vpc check and create required segmentation (this could be VPC, or Project) init_dns_zone create a dns sub-zone which will be used for cluster services exposing 5. Create cleanup function \u00b6 Create a function that deletes: kubernetes cluster vpc/project domains with dependent resources on yaml option installed:false 6. Set cloud provisioner type \u00b6 The options on provisioners could differ even inside the same cloud provider, ex: Minikube, EKS, K3s. The required set of functions is as follows: Function name What the function should do deploy_cluster Deploy kubernetes cluster itself - save result in a separate tf-state pull_kubeconfig Obtain kubeconfig from the created cluster to use in the next steps with tf/helm init_addons Install mandatory kubernetes addons (see the next step) - write the result as a separate tf-state deploy_apps Deploy/reconcile other applications from user's repository, from the defined folder output_access_keys Add output with credentials URL's and other access parameters 7. Deploy mandatory addons \u00b6 Deploy mandatory addons for the cluster ( deploy_apps ): Storage Class for creating a PVC/PV Ingress Controller to serve traffic inside the cluster CertManager to create and manage certificates ExternalDNS to create DNS records ArgoCD to deploy an application 8. Test build \u00b6 To test whether the build is successful, deploy a sample application from /kubernetes/apps/samples/ .","title":"How to add a new Provider or Provisioner"},{"location":"add-provider-provisioner/#adding-a-new-provider-or-provisioner","text":"1. Create cluster manifest 2. Create Github Action and Gitlab pipeline 3. Define required treatment in main function 4. Define cloud related functions 5. Create cleanup function 6. Set cloud provisioner type 7. Deploy mandatory addons 8. Test build","title":"Adding a New Provider or Provisioner "},{"location":"add-provider-provisioner/#1-create-cluster-manifest","text":"Create a sample yaml cluster manifest with declaration for possible options in yaml and its naming. Naming for the options should be aligned with correspondent names in terraform provider or module. Set the list of options in OPTIONS.md .","title":"1. Create cluster manifest"},{"location":"add-provider-provisioner/#2-create-github-action-and-gitlab-pipeline","text":"Defining required cloud authentication credentials, like Username/Password, Personal Access Tokens, or Access files.","title":"2. Create Github Action and Gitlab pipeline"},{"location":"add-provider-provisioner/#3-define-required-treatment-in-main-function","text":"Example: # entrypoint.sh digitalocean ) DEBUG \"Cloud Provider: DigitalOcean\" ;;","title":"3. Define required treatment in main function"},{"location":"add-provider-provisioner/#4-define-cloud-related-functions","text":"Define cloud related functions in a dedicated file, for ex: /bin/digitalocean_common.sh . Required functions: Function name What the function should do init_state_bucket check if the bucket exists and create a storage for terraform state init_vpc check and create required segmentation (this could be VPC, or Project) init_dns_zone create a dns sub-zone which will be used for cluster services exposing","title":"4. Define cloud related functions"},{"location":"add-provider-provisioner/#5-create-cleanup-function","text":"Create a function that deletes: kubernetes cluster vpc/project domains with dependent resources on yaml option installed:false","title":"5. Create cleanup function"},{"location":"add-provider-provisioner/#6-set-cloud-provisioner-type","text":"The options on provisioners could differ even inside the same cloud provider, ex: Minikube, EKS, K3s. The required set of functions is as follows: Function name What the function should do deploy_cluster Deploy kubernetes cluster itself - save result in a separate tf-state pull_kubeconfig Obtain kubeconfig from the created cluster to use in the next steps with tf/helm init_addons Install mandatory kubernetes addons (see the next step) - write the result as a separate tf-state deploy_apps Deploy/reconcile other applications from user's repository, from the defined folder output_access_keys Add output with credentials URL's and other access parameters","title":"6. Set cloud provisioner type"},{"location":"add-provider-provisioner/#7-deploy-mandatory-addons","text":"Deploy mandatory addons for the cluster ( deploy_apps ): Storage Class for creating a PVC/PV Ingress Controller to serve traffic inside the cluster CertManager to create and manage certificates ExternalDNS to create DNS records ArgoCD to deploy an application","title":"7. Deploy mandatory addons"},{"location":"add-provider-provisioner/#8-test-build","text":"To test whether the build is successful, deploy a sample application from /kubernetes/apps/samples/ .","title":"8. Test build"},{"location":"argocd-github-auth/","text":"Adding Auth Provider \u00b6 For details please see the Argo CD documentation . Edit ArgoCD configmap and set the dex.config : kubectl edit configmap argocd-cm -n argocd clientID and clientSecret should be obtained while creating a Github Oauth App: dex.config : | connectors: - type: github id: github name: GitHub config: clientID: 00000000000000000000 clientSecret: 000000000000000000000000000000000000 orgs: - name: shalb Mapping Group Permissions \u00b6 For details please see the Argo CD documentation . After login you'll receive authentication with login, ex: voa@shalb.com , with your GitHub group setting, ex: shalb:dev So you can define its permission in ArgoCD project manifest: apiVersion : argoproj.io/v1alpha1 kind : AppProject spec : clusterResourceWhitelist : - group : '*' kind : '*' destinations : - namespace : '*' server : '*' sourceRepos : - '*' roles : - description : Read-only privileges to default groups : - shalb:dev name : read-only policies : - p, proj:default:read-only, applications, get, default/*, allow","title":"Enable GitHub auth in ArgoCD"},{"location":"argocd-github-auth/#adding-auth-provider","text":"For details please see the Argo CD documentation . Edit ArgoCD configmap and set the dex.config : kubectl edit configmap argocd-cm -n argocd clientID and clientSecret should be obtained while creating a Github Oauth App: dex.config : | connectors: - type: github id: github name: GitHub config: clientID: 00000000000000000000 clientSecret: 000000000000000000000000000000000000 orgs: - name: shalb","title":"Adding Auth Provider"},{"location":"argocd-github-auth/#mapping-group-permissions","text":"For details please see the Argo CD documentation . After login you'll receive authentication with login, ex: voa@shalb.com , with your GitHub group setting, ex: shalb:dev So you can define its permission in ArgoCD project manifest: apiVersion : argoproj.io/v1alpha1 kind : AppProject spec : clusterResourceWhitelist : - group : '*' kind : '*' destinations : - namespace : '*' server : '*' sourceRepos : - '*' roles : - description : Read-only privileges to default groups : - shalb:dev name : read-only policies : - p, proj:default:read-only, applications, get, default/*, allow","title":"Mapping Group Permissions"},{"location":"aws-iam-permissions/","text":"AWS IAM permissions \u00b6 This document explains how to create or update aws_policy.json . Requirements Collect logs Run installation and destroy Copy logs Parse logs Copy logs parsing script Get API calls with service Get API calls with service and request Create policy Requirements \u00b6 Create admin account in AWS with Access Key . See the AWS documentation . Install aws-cli . See AWS documentation . Configure aws-cli aws configure Create S3 bucket aws s3 mb s3://my-cloud-logs Configure Cloud Trail. Any region is OK. See AWS documentation . Needed options: Name: any ( example: my-cloud-logs ) Trail settings: Apply trail to all regions: Yes Management events: Read/Write events All Data events: Select all S3 buckets in your account: Read, Write Storage location: S3 bucket: my-cloud-logs Collect logs \u00b6 Run installation and destroy \u00b6 See README.md . Copy logs \u00b6 Replace MY-... by your account, region, date. Copy the logs from region us-east-1 , because global API calls are logged in this region. mkdir ./aws_logs/ aws s3 sync s3://my-cloud-logs/AWSLogs/MY-ACCOUNT-ID/CloudTrail/MY-REGION/MY-YEAR/MY-MONTH/MY-DAY/ ./aws_logs/ aws s3 sync s3://my-cloud-logs/AWSLogs/MY-ACCOUNT-ID/CloudTrail/us-east-1/MY-YEAR/MY-MONTH/MY-DAY/ ./aws_logs/ gzip -d ./aws_logs/*.gz Parse logs \u00b6 Copy logs parsing script \u00b6 curl https://raw.githubusercontent.com/shalb/cluster.dev/master/install/aws_logs_parser.py > aws_logs_parser.py Get API calls with service \u00b6 Replace MY-IP by your IP address, which is used to deploy the cluster. ./aws_logs_parser.py --ip_address = MY-IP | awk -F \"|\" '{print $1 $2}' | sort -u | less -Ni Get API calls with service and request \u00b6 Replace MY-IP by your IP address, which is used to deploy the cluster. ./aws_logs_parser.py --ip_address = MY-IP | sort -u | less -Ni Create policy \u00b6 Open visual policy editor and add needed permissions regarding the output of the script . Save new policy time to time if it has many records, to prevent results from being lost. Check out its JSON version and save it to aws_policy.json . Push JSON version to repo.","title":"AWS IAM Permissions"},{"location":"aws-iam-permissions/#aws-iam-permissions","text":"This document explains how to create or update aws_policy.json . Requirements Collect logs Run installation and destroy Copy logs Parse logs Copy logs parsing script Get API calls with service Get API calls with service and request Create policy","title":"AWS IAM permissions"},{"location":"aws-iam-permissions/#requirements","text":"Create admin account in AWS with Access Key . See the AWS documentation . Install aws-cli . See AWS documentation . Configure aws-cli aws configure Create S3 bucket aws s3 mb s3://my-cloud-logs Configure Cloud Trail. Any region is OK. See AWS documentation . Needed options: Name: any ( example: my-cloud-logs ) Trail settings: Apply trail to all regions: Yes Management events: Read/Write events All Data events: Select all S3 buckets in your account: Read, Write Storage location: S3 bucket: my-cloud-logs","title":"Requirements"},{"location":"aws-iam-permissions/#collect-logs","text":"","title":"Collect logs"},{"location":"aws-iam-permissions/#run-installation-and-destroy","text":"See README.md .","title":"Run installation and destroy"},{"location":"aws-iam-permissions/#copy-logs","text":"Replace MY-... by your account, region, date. Copy the logs from region us-east-1 , because global API calls are logged in this region. mkdir ./aws_logs/ aws s3 sync s3://my-cloud-logs/AWSLogs/MY-ACCOUNT-ID/CloudTrail/MY-REGION/MY-YEAR/MY-MONTH/MY-DAY/ ./aws_logs/ aws s3 sync s3://my-cloud-logs/AWSLogs/MY-ACCOUNT-ID/CloudTrail/us-east-1/MY-YEAR/MY-MONTH/MY-DAY/ ./aws_logs/ gzip -d ./aws_logs/*.gz","title":"Copy logs"},{"location":"aws-iam-permissions/#parse-logs","text":"","title":"Parse logs"},{"location":"aws-iam-permissions/#copy-logs-parsing-script","text":"curl https://raw.githubusercontent.com/shalb/cluster.dev/master/install/aws_logs_parser.py > aws_logs_parser.py","title":"Copy logs parsing script"},{"location":"aws-iam-permissions/#get-api-calls-with-service","text":"Replace MY-IP by your IP address, which is used to deploy the cluster. ./aws_logs_parser.py --ip_address = MY-IP | awk -F \"|\" '{print $1 $2}' | sort -u | less -Ni","title":"Get API calls with service"},{"location":"aws-iam-permissions/#get-api-calls-with-service-and-request","text":"Replace MY-IP by your IP address, which is used to deploy the cluster. ./aws_logs_parser.py --ip_address = MY-IP | sort -u | less -Ni","title":"Get API calls with service and request"},{"location":"aws-iam-permissions/#create-policy","text":"Open visual policy editor and add needed permissions regarding the output of the script . Save new policy time to time if it has many records, to prevent results from being lost. Check out its JSON version and save it to aws_policy.json . Push JSON version to repo.","title":"Create policy"},{"location":"bash-logging/","text":"Bash Logger \u00b6 Bash Logger designed to incorporate PSR-3 compliance. Using Bash Logger \u00b6 source the logging.sh script at the beginning of any Bash program. #!/bin/bash source ./bin/logging.sh INFO \"This is a test info log\" Function names are in CAPS as not to conflict with the info function and alert aliases. An Overview of Bash Logger \u00b6 Colorized Output \u00b6 Logging Levels \u00b6 Bash Logger supports the logging levels described by RFC 5424 . DEBUG Detailed debug information. INFO Interesting events. Examples: User logs in, SQL logs. NOTICE Normal but significant events. WARNING Exceptional occurrences that are not errors. Examples: Use of deprecated APIs, poor use of an API, undesirable things that are not necessarily wrong. ERROR Runtime errors that do not require immediate action but should typically be logged and monitored. CRITICAL Critical conditions. Example: Application component unavailable, unexpected exception. ALERT Action must be taken immediately. Example: Entire website down, database unavailable, etc. This should trigger the SMS alerts and wake you up. EMERGENCY Emergency: system is unusable. You can set minimum log level by VERBOSE_LVL environment variable in workflow definition ( .github/workflows/*.yaml ). Default to INFO env : VERBOSE_LVL : INFO Handlers \u00b6 By default: Logs are displayed in colour error level logs and above exit with an error code The colours, logfile, default behavior, and log-level behavior can all be overwritten, see examples.sh for examples. Usage information \u00b6 Basic usage: DEBUG \"log message\" Redefine log format: DEBUG \"msg\" \"%DATE - %MESSAGE\" Existing format parameters: %MESSAGE - log message %LEVEL - log level %PID - Process ID %DATE - Date and/or time %FUNC_TRACE - Function trace from main function to executed in format function_with_trouble <- some_func <- main Redefine date format: DEBUG \"msg\" \"+%T\" Date format same as in date command Remove from function trace last function name: DEBUG \"msg\" \"\" \"\" 1 This need for logging inside wrapper function. Can be set to number greater or equal 0 . run_cmd() \u00b6 source ./bin/logging.sh run_cmd \"terraform init\" Function run_cmd run specified command into wrapper, that print command output only when: Error happened VERBOSE_LVL=DEBUG Arguments: command - command that should be executed inside wrapper bash_opts - additional shell options fail_on_err - interpret not 0 exit code as error or not. Boolean. By default - true . enable_log_timeout - Print all logs for command after timeout. Useful for non-DEBUG log levels. By default - 300 seconds (5 min) Contributors \u00b6 Dean Rather Fred Palmer Maksym Vlasov Initially, fork of bash-logger , whish is fork of log4bash .","title":"Bash Logger"},{"location":"bash-logging/#bash-logger","text":"Bash Logger designed to incorporate PSR-3 compliance.","title":"Bash Logger"},{"location":"bash-logging/#using-bash-logger","text":"source the logging.sh script at the beginning of any Bash program. #!/bin/bash source ./bin/logging.sh INFO \"This is a test info log\" Function names are in CAPS as not to conflict with the info function and alert aliases.","title":"Using Bash Logger"},{"location":"bash-logging/#an-overview-of-bash-logger","text":"","title":"An Overview of Bash Logger"},{"location":"bash-logging/#colorized-output","text":"","title":"Colorized Output"},{"location":"bash-logging/#logging-levels","text":"Bash Logger supports the logging levels described by RFC 5424 . DEBUG Detailed debug information. INFO Interesting events. Examples: User logs in, SQL logs. NOTICE Normal but significant events. WARNING Exceptional occurrences that are not errors. Examples: Use of deprecated APIs, poor use of an API, undesirable things that are not necessarily wrong. ERROR Runtime errors that do not require immediate action but should typically be logged and monitored. CRITICAL Critical conditions. Example: Application component unavailable, unexpected exception. ALERT Action must be taken immediately. Example: Entire website down, database unavailable, etc. This should trigger the SMS alerts and wake you up. EMERGENCY Emergency: system is unusable. You can set minimum log level by VERBOSE_LVL environment variable in workflow definition ( .github/workflows/*.yaml ). Default to INFO env : VERBOSE_LVL : INFO","title":"Logging Levels"},{"location":"bash-logging/#handlers","text":"By default: Logs are displayed in colour error level logs and above exit with an error code The colours, logfile, default behavior, and log-level behavior can all be overwritten, see examples.sh for examples.","title":"Handlers"},{"location":"bash-logging/#usage-information","text":"Basic usage: DEBUG \"log message\" Redefine log format: DEBUG \"msg\" \"%DATE - %MESSAGE\" Existing format parameters: %MESSAGE - log message %LEVEL - log level %PID - Process ID %DATE - Date and/or time %FUNC_TRACE - Function trace from main function to executed in format function_with_trouble <- some_func <- main Redefine date format: DEBUG \"msg\" \"+%T\" Date format same as in date command Remove from function trace last function name: DEBUG \"msg\" \"\" \"\" 1 This need for logging inside wrapper function. Can be set to number greater or equal 0 .","title":"Usage information"},{"location":"bash-logging/#run_cmd","text":"source ./bin/logging.sh run_cmd \"terraform init\" Function run_cmd run specified command into wrapper, that print command output only when: Error happened VERBOSE_LVL=DEBUG Arguments: command - command that should be executed inside wrapper bash_opts - additional shell options fail_on_err - interpret not 0 exit code as error or not. Boolean. By default - true . enable_log_timeout - Print all logs for command after timeout. Useful for non-DEBUG log levels. By default - 300 seconds (5 min)","title":"run_cmd()"},{"location":"bash-logging/#contributors","text":"Dean Rather Fred Palmer Maksym Vlasov Initially, fork of bash-logger , whish is fork of log4bash .","title":"Contributors"},{"location":"cleanup/","text":"Cleanup \u00b6 To shutdown the cluster and remove all associated resources: Open .cluster.dev/ directory in your repo. In each manifest set cluster.installed to false . Commit and push changes. Open GitHub Action output to see the removal status. After successful removal, you can safely delete cluster manifest file from .cluster.dev/ directory.","title":"Cleanup"},{"location":"cleanup/#cleanup","text":"To shutdown the cluster and remove all associated resources: Open .cluster.dev/ directory in your repo. In each manifest set cluster.installed to false . Commit and push changes. Open GitHub Action output to see the removal status. After successful removal, you can safely delete cluster manifest file from .cluster.dev/ directory.","title":"Cleanup"},{"location":"development/","text":"","title":"Development"},{"location":"getting-started-aws/","text":"Getting started on AWS \u00b6 Deploying to AWS \u00b6 Create a separate repository for the infrastructure code that will be managed by cluster.dev in GitHub. This repo will host code for your clusters, deployments, applications and other resources. Clone the repo locally: git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY cd YOUR-REPOSITORY Next steps should be done inside that repo. Create a new AWS user with limited access in IAM and apply the policy aws_policy.json . For details on how to gather required permissions see the AWS IAM permissions . The resulting pair of access keys should look like: AWS_ACCESS_KEY_ID = ATIAAJSXDBUVOQ4JR AWS_SECRET_ACCESS_KEY = SuperAwsSecret Add credentials to you repo Secrets under GitHub's repo setting Settings \u2192 Secrets , the path should look like https://github.com/MY_USER/MY_REPO_NAME/settings/secrets : In your repo, create a Github workflow file: .github/workflows/main.yml and cluster.dev example manifest: .cluster.dev/aws-minikube.yaml with the cluster definition. Or download example files to your local repo clone using the next commands: # Sample with Minikube cluster export RELEASE = v0.3.3 mkdir -p .github/workflows/ && wget -O .github/workflows/main.yml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.github/workflows/aws.yml\" mkdir -p .cluster.dev/ && wget -O .cluster.dev/aws-minikube.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.cluster.dev/aws-minikube.yaml\" In the cluster manifest (.cluster.dev/aws-minikube.yaml) you can set your own Route53 DNS zone. If you don't have any hosted public zone you can set just domain: cluster.dev and we will create it for you. Or you can create it manually with instructions from AWS Website . You can change all other parameters or leave default values in the cluster manifest. Leave the Github workflow file .github/workflows/main.yml as is. Copy a sample of ArgoCD Applications from /kubernetes/apps/samples and Helm chart samples from /kubernetes/charts/wordpress to the same paths into your repo. Or download application samples directly to local repo clone with the commands: export RELEASE = v0.3.3 # Create directory and place ArgoCD applications inside mkdir -p kubernetes/apps/samples && wget -O kubernetes/apps/samples/helm-all-in-app.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-all-in-app.yaml\" wget -O kubernetes/apps/samples/helm-dependency.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-dependency.yaml\" wget -O kubernetes/apps/samples/raw-manifest.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/raw-manifest.yaml\" # Download sample chart which with own values.yaml mkdir -p kubernetes/charts/wordpress && wget -O kubernetes/charts/wordpress/Chart.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/Chart.yaml\" wget -O kubernetes/charts/wordpress/requirements.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/requirements.yaml\" wget -O kubernetes/charts/wordpress/values.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/values.yaml\" Define path to ArgoCD apps in the cluster manifest : apps : - /kubernetes/apps/samples Commit and Push files to your repo. Set the cluster to installed: true , commit, push and follow the Github Action execution status, the path should look like https://github.com/MY_USER/MY_REPO_NAME/actions . In the GitHub action output you'll receive access instructions to your cluster and services: Voil\u00e0! You receive GitOps managed infrastructure in code. So now you can deploy applications, create more clusters, integrate with CI systems, experiment with the new features and everything else from Git without leaving your IDE.","title":"Getting started on AWS"},{"location":"getting-started-aws/#getting-started-on-aws","text":"","title":"Getting started on AWS"},{"location":"getting-started-aws/#deploying-to-aws","text":"Create a separate repository for the infrastructure code that will be managed by cluster.dev in GitHub. This repo will host code for your clusters, deployments, applications and other resources. Clone the repo locally: git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY cd YOUR-REPOSITORY Next steps should be done inside that repo. Create a new AWS user with limited access in IAM and apply the policy aws_policy.json . For details on how to gather required permissions see the AWS IAM permissions . The resulting pair of access keys should look like: AWS_ACCESS_KEY_ID = ATIAAJSXDBUVOQ4JR AWS_SECRET_ACCESS_KEY = SuperAwsSecret Add credentials to you repo Secrets under GitHub's repo setting Settings \u2192 Secrets , the path should look like https://github.com/MY_USER/MY_REPO_NAME/settings/secrets : In your repo, create a Github workflow file: .github/workflows/main.yml and cluster.dev example manifest: .cluster.dev/aws-minikube.yaml with the cluster definition. Or download example files to your local repo clone using the next commands: # Sample with Minikube cluster export RELEASE = v0.3.3 mkdir -p .github/workflows/ && wget -O .github/workflows/main.yml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.github/workflows/aws.yml\" mkdir -p .cluster.dev/ && wget -O .cluster.dev/aws-minikube.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.cluster.dev/aws-minikube.yaml\" In the cluster manifest (.cluster.dev/aws-minikube.yaml) you can set your own Route53 DNS zone. If you don't have any hosted public zone you can set just domain: cluster.dev and we will create it for you. Or you can create it manually with instructions from AWS Website . You can change all other parameters or leave default values in the cluster manifest. Leave the Github workflow file .github/workflows/main.yml as is. Copy a sample of ArgoCD Applications from /kubernetes/apps/samples and Helm chart samples from /kubernetes/charts/wordpress to the same paths into your repo. Or download application samples directly to local repo clone with the commands: export RELEASE = v0.3.3 # Create directory and place ArgoCD applications inside mkdir -p kubernetes/apps/samples && wget -O kubernetes/apps/samples/helm-all-in-app.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-all-in-app.yaml\" wget -O kubernetes/apps/samples/helm-dependency.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-dependency.yaml\" wget -O kubernetes/apps/samples/raw-manifest.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/raw-manifest.yaml\" # Download sample chart which with own values.yaml mkdir -p kubernetes/charts/wordpress && wget -O kubernetes/charts/wordpress/Chart.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/Chart.yaml\" wget -O kubernetes/charts/wordpress/requirements.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/requirements.yaml\" wget -O kubernetes/charts/wordpress/values.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/values.yaml\" Define path to ArgoCD apps in the cluster manifest : apps : - /kubernetes/apps/samples Commit and Push files to your repo. Set the cluster to installed: true , commit, push and follow the Github Action execution status, the path should look like https://github.com/MY_USER/MY_REPO_NAME/actions . In the GitHub action output you'll receive access instructions to your cluster and services: Voil\u00e0! You receive GitOps managed infrastructure in code. So now you can deploy applications, create more clusters, integrate with CI systems, experiment with the new features and everything else from Git without leaving your IDE.","title":"Deploying to AWS"},{"location":"getting-started-digitalocean/","text":"Getting started on DigitalOcean \u00b6 Deploying to DigitalOcean \u00b6 Create a separate repository for the infrastructure code that will be managed by cluster.dev in GitHub. This repo will host code for your clusters, deployments, applications and other resources. Clone the repo locally: git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY cd YOUR-REPOSITORY Next steps should be done inside that repo. Login to your DO account. You can create a default VPC inside your account: Manage->Networking->VPC-Create VPC Network . Next, you need to generate a DO API token and DO Spaces keys. To generate the API token, see the DO documentation . The token should like: DIGITALOCEAN_TOKEN : \"83e209a810b6c1da8919fe7265b9493992929b9221444449\" To generate the DO Spaces secrets, see the DO documentation . The resulting key and secret should look like: SPACES_ACCESS_KEY_ID : \"L2Z3UN2I4R322XX56LPM\" SPACES_SECRET_ACCESS_KEY : \"njVtezJ7t2ce1nlohIFwoPHHF333mmcc2\" Add the token and Spaces keys to your repo secrets or env variables. In GitHub: Settings \u2192 Secrets , the path should look like: https://github.com/MY_USER/MY_REPO_NAME/settings/secrets : In your repo, create a Github workflow file: .github/workflows/main.yml and cluster.dev example manifest: .cluster.dev/digitalocean-k8s.yaml with the cluster definition. Or download example files to your local repo clone using the next commands: # Sample with DO Managed Kubernetes Cluster export RELEASE = v0.3.3 mkdir -p .github/workflows/ && wget -O .github/workflows/main.yml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.github/workflows/digitalocean.yml\" mkdir -p .cluster.dev/ && wget -O .cluster.dev/digitalocean-k8s.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.cluster.dev/digitalocean-k8s.yaml\" In the cluster manifest (.cluster.dev/digitalocean-k8s.yaml) you can set your own Domain Zone. If you don't have any hosted public zone you can set just domain: cluster.dev and we will create it for you. Or you can create it manually and add to your account with instructions from DO website . You can change all other parameters or leave default values in the cluster manifest. Leave the Github workflow file .github/workflows/main.yml as is. Copy sample ArgoCD Applications from /kubernetes/apps/samples and Helm chart samples from /kubernetes/charts/wordpress to the same paths into your repo. Or download application samples directly to local repo clone with the commands: export RELEASE = v0.3.3 # Create directory and place ArgoCD applications inside mkdir -p kubernetes/apps/samples && wget -O kubernetes/apps/samples/helm-all-in-app.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-all-in-app.yaml\" wget -O kubernetes/apps/samples/helm-dependency.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-dependency.yaml\" wget -O kubernetes/apps/samples/raw-manifest.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/raw-manifest.yaml\" # Download sample chart which with own values.yaml mkdir -p kubernetes/charts/wordpress && wget -O kubernetes/charts/wordpress/Chart.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/Chart.yaml\" wget -O kubernetes/charts/wordpress/requirements.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/requirements.yaml\" wget -O kubernetes/charts/wordpress/values.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/values.yaml\" Define path to ArgoCD apps in the cluster manifest : apps : - /kubernetes/apps/samples Commit and Push files to your repo. Set the cluster to installed: true , commit, push and follow the Github Action execution status, the path should look like https://github.com/MY_USER/MY_REPO_NAME/actions . In the GitHub action output you'll receive access instructions to your cluster and services: Voil\u00e0! You receive GitOps managed infrastructure in code. So now you can deploy applications, create more clusters, integrate with CI systems, experiment with the new features and everything else from Git without leaving your IDE.","title":"Getting started on DigitalOcean"},{"location":"getting-started-digitalocean/#getting-started-on-digitalocean","text":"","title":"Getting started on DigitalOcean"},{"location":"getting-started-digitalocean/#deploying-to-digitalocean","text":"Create a separate repository for the infrastructure code that will be managed by cluster.dev in GitHub. This repo will host code for your clusters, deployments, applications and other resources. Clone the repo locally: git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY cd YOUR-REPOSITORY Next steps should be done inside that repo. Login to your DO account. You can create a default VPC inside your account: Manage->Networking->VPC-Create VPC Network . Next, you need to generate a DO API token and DO Spaces keys. To generate the API token, see the DO documentation . The token should like: DIGITALOCEAN_TOKEN : \"83e209a810b6c1da8919fe7265b9493992929b9221444449\" To generate the DO Spaces secrets, see the DO documentation . The resulting key and secret should look like: SPACES_ACCESS_KEY_ID : \"L2Z3UN2I4R322XX56LPM\" SPACES_SECRET_ACCESS_KEY : \"njVtezJ7t2ce1nlohIFwoPHHF333mmcc2\" Add the token and Spaces keys to your repo secrets or env variables. In GitHub: Settings \u2192 Secrets , the path should look like: https://github.com/MY_USER/MY_REPO_NAME/settings/secrets : In your repo, create a Github workflow file: .github/workflows/main.yml and cluster.dev example manifest: .cluster.dev/digitalocean-k8s.yaml with the cluster definition. Or download example files to your local repo clone using the next commands: # Sample with DO Managed Kubernetes Cluster export RELEASE = v0.3.3 mkdir -p .github/workflows/ && wget -O .github/workflows/main.yml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.github/workflows/digitalocean.yml\" mkdir -p .cluster.dev/ && wget -O .cluster.dev/digitalocean-k8s.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /.cluster.dev/digitalocean-k8s.yaml\" In the cluster manifest (.cluster.dev/digitalocean-k8s.yaml) you can set your own Domain Zone. If you don't have any hosted public zone you can set just domain: cluster.dev and we will create it for you. Or you can create it manually and add to your account with instructions from DO website . You can change all other parameters or leave default values in the cluster manifest. Leave the Github workflow file .github/workflows/main.yml as is. Copy sample ArgoCD Applications from /kubernetes/apps/samples and Helm chart samples from /kubernetes/charts/wordpress to the same paths into your repo. Or download application samples directly to local repo clone with the commands: export RELEASE = v0.3.3 # Create directory and place ArgoCD applications inside mkdir -p kubernetes/apps/samples && wget -O kubernetes/apps/samples/helm-all-in-app.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-all-in-app.yaml\" wget -O kubernetes/apps/samples/helm-dependency.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/helm-dependency.yaml\" wget -O kubernetes/apps/samples/raw-manifest.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/apps/samples/raw-manifest.yaml\" # Download sample chart which with own values.yaml mkdir -p kubernetes/charts/wordpress && wget -O kubernetes/charts/wordpress/Chart.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/Chart.yaml\" wget -O kubernetes/charts/wordpress/requirements.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/requirements.yaml\" wget -O kubernetes/charts/wordpress/values.yaml \"https://raw.githubusercontent.com/shalb/cluster.dev/ ${ RELEASE } /kubernetes/charts/wordpress/values.yaml\" Define path to ArgoCD apps in the cluster manifest : apps : - /kubernetes/apps/samples Commit and Push files to your repo. Set the cluster to installed: true , commit, push and follow the Github Action execution status, the path should look like https://github.com/MY_USER/MY_REPO_NAME/actions . In the GitHub action output you'll receive access instructions to your cluster and services: Voil\u00e0! You receive GitOps managed infrastructure in code. So now you can deploy applications, create more clusters, integrate with CI systems, experiment with the new features and everything else from Git without leaving your IDE.","title":"Deploying to DigitalOcean"},{"location":"git-provider-support/","text":"Git Provider Support \u00b6 GitHub Actions Workflow Configuration \u00b6 More examples could be found in /.github/workflows directory. # sample .github/workflows/aws.yaml on : push : # This is how you can define after what changes it should be triggered paths : - '.cluster.dev/aws-minikube.yaml' branches : - master jobs : deploy_cluster_job : runs-on : ubuntu-latest name : Cluster.dev steps : - name : Checkout Repo uses : actions/checkout@v2 - name : Reconcile Clusters id : reconcile # Here you can define what release version of action to use, # example: shalb/cluster.dev@master, shalb/cluster.dev@v0.3.3, shalb/cluster.dev@test-branch uses : shalb/cluster.dev@v0.3.3 # Here the required environment variables should be set depending on Cloud Provider env : AWS_ACCESS_KEY_ID : \"${{ secrets.AWS_ACCESS_KEY_ID }}\" AWS_SECRET_ACCESS_KEY : \"${{ secrets.AWS_SECRET_ACCESS_KEY }}\" CLUSTER_CONFIG_PATH : \"./.cluster.dev/\" # Here the debug level for the ACTION could be set (default: INFO) VERBOSE_LVL : DEBUG - name : Get the Cluster Credentials run : echo -e \"\\n\\033[1;32m${{ steps.reconcile.outputs.ssh }}\\n\\033[1;32m${{ steps.reconcile.outputs.kubeconfig }}\\n\\033[1;32m${{ steps.reconcile.outputs.argocd }}\" GitLab CI/CD Pipeline Configuration \u00b6 Full example could be found in /install/.gitlab-ci-sample.yml # Example for .gitlab-ci.yml pipeline with cluster.dev job image : docker:19.03.0 variables : DOCKER_DRIVER : overlay2 # Docker Settings DOCKER_TLS_CERTDIR : \"/certs\" CLUSTER_DEV_BRANCH : \"master\" # Define branch or release version CLUSTER_CONFIG_PATH : \"./.cluster.dev/\" # Path to manifests DIGITALOCEAN_TOKEN : \"${DIGITALOCEAN_TOKEN}\" # Environment variables depending on Cloud Provider SPACES_ACCESS_KEY_ID : \"${SPACES_ACCESS_KEY_ID}\" SPACES_SECRET_ACCESS_KEY : \"${SPACES_SECRET_ACCESS_KEY}\" services : - docker:19.03.0-dind before_script : - apk update && apk upgrade && apk add --no-cache bash git stages : - cluster-dev cluster-dev : only : refs : - master changes : - '.gitlab-ci.yml' - '.cluster.dev/**' # Path to cluster declaration manifests - '/kubernetes/apps/**' # ArgoCD application directories script : - git clone -b \"$CLUSTER_DEV_BRANCH\" https://github.com/shalb/cluster.dev.git - cd cluster.dev && docker build --no-cache -t \"cluster.dev\" . - docker run --name cluster.dev --workdir /gitlab/workspace --rm -e CI_PROJECT_PATH -e CI_PROJECT_DIR -e VERBOSE_LVL=DEBUG -e DIGITALOCEAN_TOKEN -e SPACES_ACCESS_KEY_ID -e SPACES_SECRET_ACCESS_KEY -v \"${CI_PROJECT_DIR}:/gitlab/workspace\" cluster.dev stage : cluster-dev","title":"Git Provider Support"},{"location":"git-provider-support/#git-provider-support","text":"","title":"Git Provider Support"},{"location":"git-provider-support/#github-actions-workflow-configuration","text":"More examples could be found in /.github/workflows directory. # sample .github/workflows/aws.yaml on : push : # This is how you can define after what changes it should be triggered paths : - '.cluster.dev/aws-minikube.yaml' branches : - master jobs : deploy_cluster_job : runs-on : ubuntu-latest name : Cluster.dev steps : - name : Checkout Repo uses : actions/checkout@v2 - name : Reconcile Clusters id : reconcile # Here you can define what release version of action to use, # example: shalb/cluster.dev@master, shalb/cluster.dev@v0.3.3, shalb/cluster.dev@test-branch uses : shalb/cluster.dev@v0.3.3 # Here the required environment variables should be set depending on Cloud Provider env : AWS_ACCESS_KEY_ID : \"${{ secrets.AWS_ACCESS_KEY_ID }}\" AWS_SECRET_ACCESS_KEY : \"${{ secrets.AWS_SECRET_ACCESS_KEY }}\" CLUSTER_CONFIG_PATH : \"./.cluster.dev/\" # Here the debug level for the ACTION could be set (default: INFO) VERBOSE_LVL : DEBUG - name : Get the Cluster Credentials run : echo -e \"\\n\\033[1;32m${{ steps.reconcile.outputs.ssh }}\\n\\033[1;32m${{ steps.reconcile.outputs.kubeconfig }}\\n\\033[1;32m${{ steps.reconcile.outputs.argocd }}\"","title":"GitHub Actions Workflow Configuration"},{"location":"git-provider-support/#gitlab-cicd-pipeline-configuration","text":"Full example could be found in /install/.gitlab-ci-sample.yml # Example for .gitlab-ci.yml pipeline with cluster.dev job image : docker:19.03.0 variables : DOCKER_DRIVER : overlay2 # Docker Settings DOCKER_TLS_CERTDIR : \"/certs\" CLUSTER_DEV_BRANCH : \"master\" # Define branch or release version CLUSTER_CONFIG_PATH : \"./.cluster.dev/\" # Path to manifests DIGITALOCEAN_TOKEN : \"${DIGITALOCEAN_TOKEN}\" # Environment variables depending on Cloud Provider SPACES_ACCESS_KEY_ID : \"${SPACES_ACCESS_KEY_ID}\" SPACES_SECRET_ACCESS_KEY : \"${SPACES_SECRET_ACCESS_KEY}\" services : - docker:19.03.0-dind before_script : - apk update && apk upgrade && apk add --no-cache bash git stages : - cluster-dev cluster-dev : only : refs : - master changes : - '.gitlab-ci.yml' - '.cluster.dev/**' # Path to cluster declaration manifests - '/kubernetes/apps/**' # ArgoCD application directories script : - git clone -b \"$CLUSTER_DEV_BRANCH\" https://github.com/shalb/cluster.dev.git - cd cluster.dev && docker build --no-cache -t \"cluster.dev\" . - docker run --name cluster.dev --workdir /gitlab/workspace --rm -e CI_PROJECT_PATH -e CI_PROJECT_DIR -e VERBOSE_LVL=DEBUG -e DIGITALOCEAN_TOKEN -e SPACES_ACCESS_KEY_ID -e SPACES_SECRET_ACCESS_KEY -v \"${CI_PROJECT_DIR}:/gitlab/workspace\" cluster.dev stage : cluster-dev","title":"GitLab CI/CD Pipeline Configuration"},{"location":"release-new-version/","text":"Release new cluster.dev version \u00b6 Create new branch or make up-to-date working branch with main branch. Replace all needed mentions of previous version by new one using your code editor. Required: change image version in action.yaml file. It will be used by Github Action. Add version bumping commit. After merging PR, create new release from the main branch. In the release, describe all changes made from the previous release. Check that docker image successfully builded here .","title":"How to create a new Release"},{"location":"release-new-version/#release-new-clusterdev-version","text":"Create new branch or make up-to-date working branch with main branch. Replace all needed mentions of previous version by new one using your code editor. Required: change image version in action.yaml file. It will be used by Github Action. Add version bumping commit. After merging PR, create new release from the main branch. In the release, describe all changes made from the previous release. Check that docker image successfully builded here .","title":"Release new cluster.dev version"},{"location":"run-reconciler-locally/","text":"Run reconciler locally \u00b6 For quick development-and-test loop you'd like run reconciler locally, not in CD. For this, needed: Install Golang 1.14 or higher Install Terraform 0.12 Export your Cloud credentials export AWS_ACCESS_KEY_ID = ATIAAJSXDBUVOQ4JR export AWS_SECRET_ACCESS_KEY = SuperAwsSecret Build reconciler cd cluster.dev/ make Run reconciler on needed cluster-dev manifest ./bin/reconciler --config .cluster.dev/aws-minikube.yaml --log-level debug","title":"How to run reconciler locally"},{"location":"run-reconciler-locally/#run-reconciler-locally","text":"For quick development-and-test loop you'd like run reconciler locally, not in CD. For this, needed: Install Golang 1.14 or higher Install Terraform 0.12 Export your Cloud credentials export AWS_ACCESS_KEY_ID = ATIAAJSXDBUVOQ4JR export AWS_SECRET_ACCESS_KEY = SuperAwsSecret Build reconciler cd cluster.dev/ make Run reconciler on needed cluster-dev manifest ./bin/reconciler --config .cluster.dev/aws-minikube.yaml --log-level debug","title":"Run reconciler locally"},{"location":"style-guide/","text":"Style guide \u00b6 For better experience, we recommend using VS Code - we have a list of recommended extensions to prevent many common errors, improve code and save time. We use .editorconfig . It fixes basic mistakes on every file saving. And please install pre-commit-terraform with all its dependencies. It checks all changed files when you run git commit for more complex problems and tries to fix them for you. Bash \u00b6 Firstly, please install shellcheck to have vscode-shellcheck extension working properly. We use Google Style Guide . Terraform \u00b6 We use Terraform Best Practices.com code style and conceptions. Autogenerated Documentation \u00b6 For the successful module documentation initialization, you need to create README.md with: <!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK --> <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK --> It is needed for terraform-docs hooks. The hook rewrites all the things inside with every .tf file change. Then run pre-commit run --all-files or make some changes in any .tf file in the same dir (for ex. variable \"name\" { -> variable \"name\"{ ). Terraform code structure \u00b6 If you'd like to know how we prefer to structure the tf-code, please see this article .","title":"Style-guide"},{"location":"style-guide/#style-guide","text":"For better experience, we recommend using VS Code - we have a list of recommended extensions to prevent many common errors, improve code and save time. We use .editorconfig . It fixes basic mistakes on every file saving. And please install pre-commit-terraform with all its dependencies. It checks all changed files when you run git commit for more complex problems and tries to fix them for you.","title":"Style guide"},{"location":"style-guide/#bash","text":"Firstly, please install shellcheck to have vscode-shellcheck extension working properly. We use Google Style Guide .","title":"Bash"},{"location":"style-guide/#terraform","text":"We use Terraform Best Practices.com code style and conceptions.","title":"Terraform"},{"location":"style-guide/#autogenerated-documentation","text":"For the successful module documentation initialization, you need to create README.md with: <!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK --> <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK --> It is needed for terraform-docs hooks. The hook rewrites all the things inside with every .tf file change. Then run pre-commit run --all-files or make some changes in any .tf file in the same dir (for ex. variable \"name\" { -> variable \"name\"{ ).","title":"Autogenerated Documentation"},{"location":"style-guide/#terraform-code-structure","text":"If you'd like to know how we prefer to structure the tf-code, please see this article .","title":"Terraform code structure"},{"location":"terraform-code-structure/","text":"Filenames \u00b6 init.tf main.tf outputs.tf vars.tf README.md Put the .yaml , .sh and other module-related files to subdir(s), ./templates for instance. Also, you can check our style-guide for other best practices. init.tf \u00b6 Sort terraform_remote_state alphabetically. # # Init # terraform { required_version = \"> = X . Y . Z\" # minimum required Terraform version. Use the oldest version if possible. required_providers { _ = \"~> X.Y\" # version of the used provider, pin only MAJOR.MINOR. Use the newest version if possible. } backend \"_\" {} } provider \"_\" {} # # Remote states for import # data \"terraform_remote_state\" \"\" {} main.tf \u00b6 Sort in logic order. Sort alphabetically when possible. # # Get/transform exit data # locals {} data \"_\" \"_\" {} resource \"_\" \"_\" {} module \"_\" {} # # Create resources # resource \"_\" \"_\" {} module \"_\" {} outputs.tf \u00b6 Sort alphabetically. output \"_\" { value = _ description = \"\" } vars.tf \u00b6 Sort alphabetically. variable \"_\" { type = _ description = \"_\" } README.md \u00b6 # Module name Describe what the module does. <!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK --> ## Inputs ## Outputs <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->","title":"Terraform modules structure"},{"location":"terraform-code-structure/#filenames","text":"init.tf main.tf outputs.tf vars.tf README.md Put the .yaml , .sh and other module-related files to subdir(s), ./templates for instance. Also, you can check our style-guide for other best practices.","title":"Filenames "},{"location":"terraform-code-structure/#inittf","text":"Sort terraform_remote_state alphabetically. # # Init # terraform { required_version = \"> = X . Y . Z\" # minimum required Terraform version. Use the oldest version if possible. required_providers { _ = \"~> X.Y\" # version of the used provider, pin only MAJOR.MINOR. Use the newest version if possible. } backend \"_\" {} } provider \"_\" {} # # Remote states for import # data \"terraform_remote_state\" \"\" {}","title":"init.tf"},{"location":"terraform-code-structure/#maintf","text":"Sort in logic order. Sort alphabetically when possible. # # Get/transform exit data # locals {} data \"_\" \"_\" {} resource \"_\" \"_\" {} module \"_\" {} # # Create resources # resource \"_\" \"_\" {} module \"_\" {}","title":"main.tf"},{"location":"terraform-code-structure/#outputstf","text":"Sort alphabetically. output \"_\" { value = _ description = \"\" }","title":"outputs.tf"},{"location":"terraform-code-structure/#varstf","text":"Sort alphabetically. variable \"_\" { type = _ description = \"_\" }","title":"vars.tf"},{"location":"terraform-code-structure/#readmemd","text":"# Module name Describe what the module does. <!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK --> ## Inputs ## Outputs <!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->","title":"README.md"},{"location":"design/cli-installer-design/","text":"Deploy Tool Dialogs Design \u00b6 We need to create a tool which will simplify installation process for most of users. The main features: 1. Create or reuse infrastructure repo within git hosting provider. 2. Select and create cloud user and required permissions. 3. Populate repo with sample files. 4. Edit view for the first cluster config. 5. Commit all code and install the cluster. 6. Display credentials with output. Design requirements: - cli should be ran as dialog by default - as an option cli could receive parameters as command line options - delivered as binary and as a docker container - ability to run on CI/CD systems - if possible re-usable functions to be used in Web version Sample Workflow \u00b6 $ ./cluster-dev install CD1: Hi, we gonna install create an infrastructure for you. ! if command spawned inside existing infrastructure repo, inform about it and propose to skip CD1-CD4 steps, ex: delete all existing configuration [yes/NO]? CD2: As this is a GitOps approach we need to start with the repo, please select your Git hosting: > GitHub > GitLab > BitBucket ! on selection credentials checked and required setting requested as user/password CD3: Now we need to create Github/GitLab/Bitbucket repo, or use existing: [infrastructure] <-editable ! repo created by terraform or selected existing and tested access to it CD4: So, I'm creating/using [infrastructure] repo and clone it locally: ! show commands: git clone github.com/voatsap/testrepo.git CD5: Ok we need to select the Cloud Provider for your cluster: > AWS > Google > DO ! credentials checked and required setting installed with profile selection. CD6: Please select existing or we would create a separate user and role for your cluster with limited permissions: [cluster-dev-user] <- editable ! run script to create/use user and or grant required permissions by role CD7: Now I'm populating sample files to your repo: ! show commands: cd /tmp/ && git clone github.com/shalb/cluster-dev-samples.git cp -R /tmp/cluster-dev-samples/ ~/testrepo/ CD8: Please enter the name for your cluster: [develop] <- editable CD9: Lets edit the the cluster manifest ! EDITOR ~/testrepo/cluster.yaml with predefined cloud/provider/cluster.name from previous steps CD10: Can we push it or open an PR for review? > Commit/Push > Create a PR > Exit, I'll do this manually CD11: Execute and show outputs Cluster is ready! Visit: https//cluster-name-gitorg.cluster.dev for details.","title":"Deploy Tool Dialogs Design"},{"location":"design/cli-installer-design/#deploy-tool-dialogs-design","text":"We need to create a tool which will simplify installation process for most of users. The main features: 1. Create or reuse infrastructure repo within git hosting provider. 2. Select and create cloud user and required permissions. 3. Populate repo with sample files. 4. Edit view for the first cluster config. 5. Commit all code and install the cluster. 6. Display credentials with output. Design requirements: - cli should be ran as dialog by default - as an option cli could receive parameters as command line options - delivered as binary and as a docker container - ability to run on CI/CD systems - if possible re-usable functions to be used in Web version","title":"Deploy Tool Dialogs Design"},{"location":"design/cli-installer-design/#sample-workflow","text":"$ ./cluster-dev install CD1: Hi, we gonna install create an infrastructure for you. ! if command spawned inside existing infrastructure repo, inform about it and propose to skip CD1-CD4 steps, ex: delete all existing configuration [yes/NO]? CD2: As this is a GitOps approach we need to start with the repo, please select your Git hosting: > GitHub > GitLab > BitBucket ! on selection credentials checked and required setting requested as user/password CD3: Now we need to create Github/GitLab/Bitbucket repo, or use existing: [infrastructure] <-editable ! repo created by terraform or selected existing and tested access to it CD4: So, I'm creating/using [infrastructure] repo and clone it locally: ! show commands: git clone github.com/voatsap/testrepo.git CD5: Ok we need to select the Cloud Provider for your cluster: > AWS > Google > DO ! credentials checked and required setting installed with profile selection. CD6: Please select existing or we would create a separate user and role for your cluster with limited permissions: [cluster-dev-user] <- editable ! run script to create/use user and or grant required permissions by role CD7: Now I'm populating sample files to your repo: ! show commands: cd /tmp/ && git clone github.com/shalb/cluster-dev-samples.git cp -R /tmp/cluster-dev-samples/ ~/testrepo/ CD8: Please enter the name for your cluster: [develop] <- editable CD9: Lets edit the the cluster manifest ! EDITOR ~/testrepo/cluster.yaml with predefined cloud/provider/cluster.name from previous steps CD10: Can we push it or open an PR for review? > Commit/Push > Create a PR > Exit, I'll do this manually CD11: Execute and show outputs Cluster is ready! Visit: https//cluster-name-gitorg.cluster.dev for details.","title":"Sample Workflow"},{"location":"presentations/hashicorp-kyiv-10/","text":"Hashicorp Users Group Meetup #10 \u00b6 Cluster.dev - Why we Build own Bicycle with Terraform and Kuberentes? \u00b6 About Me \u00b6 Volodymyr Tsap , 34 Kyiv, Ukraine CTO @ SHALB.com Love: urban tourism, tech, football, electronic music. 17 years of work experience in infrastructure management. About Company \u00b6 S ecurity H igh A vailability L oad B alancing DevOPS as a Service Company Team - 15 happy SHALB'ers. We have built more than 100 production infrastructures in last 11 years . Customers in Ukraine: Ajax, Hotline, Concert.ua, Health24, Horoshop, Payop etc. Company Specializations Vectors \u00b6 - Architectural Consulting - Infrastructure as a Code (IaC) - CI/CD - Monitoring - Support 24x7 - Databases - Security Common Modern Infrastructure \u00b6 Code Hosting: Github, Gitlab, Bitbucket Cloud Provider: AWS, Azure, GCP, DO IaC: Terraform, Bash, Ansible Workload executors: Instances, Docker, Kubernetes, Lambda's, Knative/DAGs Databases and Queues: MySQL, PostgreSQL, MongoDB, ElasticSearch, Redis, Kafka, Rabbit CI/CD: Jenkins/-X, Gitlab, GitHub, CircleCI, Argo/CD, FluxCD Observability: Elastic EFK, Prometheus, Grafana, Jaeger, OpsGenie/Pagerduty Users and auth Providers: OIDC/OAuth, IAM, LDAP, Okta, GSuite, Microsoft, GH/GL/BB SSO Secrets and Certificates: Vault, Cert-manager, 1Password, SSM, Secrets manager So we decided to create a Platform for quick launch of the common infrastructure patterns. \u00b6 And name it: \"cluster.dev\" \u00b6 Cluster.dev Objectives \u00b6 Everything in code: Infrastructure, Workloads, Pipelines, Users, Dashboards, etc.. GitOps - repo as a single source of truth Well known tooling. No programming skills required Community driven tools and modules Opionated and battle tested configuration defaults Full customization and plugin extendability Shipped as all-in-one Docker Image Eating your own dog food Implementation \u00b6 Simple yaml that describes: Cloud Setting (Projects/VPC/Networking) Kubernetes cluster Core Addons Objects declarations stored in git repo Objects states stored in: Terraform state files Kuberentes objects and CRD's ArgoCD projects and applications Resources created by Installer \u00b6 GitHub Repo Cloud User IAM Role Security Credentials Microservice Initializer Resources created and reconciled by Container \u00b6 Terraform state bucket storage VPC and Networking DNS Zone and records (we provide a zone) Kubernetes cluster: Minikube Managed(EKS/DO) ClusterAPI (tbd) Kops (tbd) Deploy Pre-configured Addons: ExternalDNS Cert-Manager Nginx-Ingress ArgoCD Keycloak (tbd) Resources created and reconciled by ArgoCD \u00b6 ArgoCD itself CI/CD Runners (GitHub/Gitlab private runners) Observability (Grafana, Prometheus, EFK, Dashboards) SSO/LDAP Realms, Users, Groups, auth proxies Secret Managers Service Mesh(Istio, Linkerd) Your custom workloads (Helm, Kustomize, CRD's, raw yaml manifests) Principle Diagram \u00b6 Demo Time \u00b6 Test Repo at voatsap/testrepo New Repo at GitHub Installer at DockerHub Roadmap \u00b6 ROADMAP.md Questions? \u00b6 Thanks you! \u00b6 STAR us https://cluster.dev \u00b6","title":"Hashicorp Users Group Meetup #10"},{"location":"presentations/hashicorp-kyiv-10/#hashicorp-users-group-meetup-10","text":"","title":"Hashicorp Users Group Meetup #10"},{"location":"presentations/hashicorp-kyiv-10/#clusterdev-why-we-build-own-bicycle-with-terraform-and-kuberentes","text":"","title":"Cluster.dev - Why we Build own Bicycle with Terraform and Kuberentes?"},{"location":"presentations/hashicorp-kyiv-10/#about-me","text":"Volodymyr Tsap , 34 Kyiv, Ukraine CTO @ SHALB.com Love: urban tourism, tech, football, electronic music. 17 years of work experience in infrastructure management.","title":"About Me"},{"location":"presentations/hashicorp-kyiv-10/#about-company","text":"S ecurity H igh A vailability L oad B alancing DevOPS as a Service Company Team - 15 happy SHALB'ers. We have built more than 100 production infrastructures in last 11 years . Customers in Ukraine: Ajax, Hotline, Concert.ua, Health24, Horoshop, Payop etc.","title":"About Company"},{"location":"presentations/hashicorp-kyiv-10/#company-specializations-vectors","text":"- Architectural Consulting - Infrastructure as a Code (IaC) - CI/CD - Monitoring - Support 24x7 - Databases - Security","title":"Company Specializations Vectors"},{"location":"presentations/hashicorp-kyiv-10/#common-modern-infrastructure","text":"Code Hosting: Github, Gitlab, Bitbucket Cloud Provider: AWS, Azure, GCP, DO IaC: Terraform, Bash, Ansible Workload executors: Instances, Docker, Kubernetes, Lambda's, Knative/DAGs Databases and Queues: MySQL, PostgreSQL, MongoDB, ElasticSearch, Redis, Kafka, Rabbit CI/CD: Jenkins/-X, Gitlab, GitHub, CircleCI, Argo/CD, FluxCD Observability: Elastic EFK, Prometheus, Grafana, Jaeger, OpsGenie/Pagerduty Users and auth Providers: OIDC/OAuth, IAM, LDAP, Okta, GSuite, Microsoft, GH/GL/BB SSO Secrets and Certificates: Vault, Cert-manager, 1Password, SSM, Secrets manager","title":"Common Modern Infrastructure"},{"location":"presentations/hashicorp-kyiv-10/#so-we-decided-to-create-a-platform-for-quick-launch-of-the-common-infrastructure-patterns","text":"","title":"So we decided to create a Platform for quick launch of the common infrastructure patterns."},{"location":"presentations/hashicorp-kyiv-10/#and-name-it-clusterdev","text":"","title":"And name it: \"cluster.dev\""},{"location":"presentations/hashicorp-kyiv-10/#clusterdev-objectives","text":"Everything in code: Infrastructure, Workloads, Pipelines, Users, Dashboards, etc.. GitOps - repo as a single source of truth Well known tooling. No programming skills required Community driven tools and modules Opionated and battle tested configuration defaults Full customization and plugin extendability Shipped as all-in-one Docker Image Eating your own dog food","title":"Cluster.dev Objectives"},{"location":"presentations/hashicorp-kyiv-10/#implementation","text":"Simple yaml that describes: Cloud Setting (Projects/VPC/Networking) Kubernetes cluster Core Addons Objects declarations stored in git repo Objects states stored in: Terraform state files Kuberentes objects and CRD's ArgoCD projects and applications","title":"Implementation"},{"location":"presentations/hashicorp-kyiv-10/#resources-created-by-installer","text":"GitHub Repo Cloud User IAM Role Security Credentials Microservice Initializer","title":"Resources created by Installer"},{"location":"presentations/hashicorp-kyiv-10/#resources-created-and-reconciled-by-container","text":"Terraform state bucket storage VPC and Networking DNS Zone and records (we provide a zone) Kubernetes cluster: Minikube Managed(EKS/DO) ClusterAPI (tbd) Kops (tbd) Deploy Pre-configured Addons: ExternalDNS Cert-Manager Nginx-Ingress ArgoCD Keycloak (tbd)","title":"Resources created and reconciled by Container"},{"location":"presentations/hashicorp-kyiv-10/#resources-created-and-reconciled-by-argocd","text":"ArgoCD itself CI/CD Runners (GitHub/Gitlab private runners) Observability (Grafana, Prometheus, EFK, Dashboards) SSO/LDAP Realms, Users, Groups, auth proxies Secret Managers Service Mesh(Istio, Linkerd) Your custom workloads (Helm, Kustomize, CRD's, raw yaml manifests)","title":"Resources created and reconciled by ArgoCD"},{"location":"presentations/hashicorp-kyiv-10/#principle-diagram","text":"","title":"Principle Diagram"},{"location":"presentations/hashicorp-kyiv-10/#demo-time","text":"Test Repo at voatsap/testrepo New Repo at GitHub Installer at DockerHub","title":"Demo Time"},{"location":"presentations/hashicorp-kyiv-10/#roadmap","text":"ROADMAP.md","title":"Roadmap"},{"location":"presentations/hashicorp-kyiv-10/#questions","text":"","title":"Questions?"},{"location":"presentations/hashicorp-kyiv-10/#thanks-you","text":"","title":"Thanks you!"},{"location":"presentations/hashicorp-kyiv-10/#star-us-httpsclusterdev","text":"","title":"STAR us https://cluster.dev"}]}